{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import json\n",
    "from sharedcontrolpaper.force_sensitive_stopping_task_utils import print_means_t_test, print_effect_size_and_ci, calc_stats_ind, convert_formats_back, rename_index_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('force_sensitive_data.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "force_sensitive_stopping_task_ssrt = pd.DataFrame.from_records(loaded_data['force_sensitive_stopping_task_ssrt'])\n",
    "go_task_accuracy_before_stop_onset = pd.DataFrame.from_records(loaded_data['go_task_accuracy_before_stop_onset'])\n",
    "go_task_accuracy_after_stop_onset = pd.DataFrame.from_records(loaded_data['go_task_accuracy_after_stop_onset'])\n",
    "duration_of_inhibition = pd.DataFrame.from_records(loaded_data['duration_of_inhibition'])\n",
    "ssrt_first_half = pd.DataFrame.from_records(loaded_data['ssrt_first_half'])\n",
    "ssrt_second_half = pd.DataFrame.from_records(loaded_data['ssrt_second_half'])\n",
    "shared_control_metrics = convert_formats_back(loaded_data['shared_control_metrics'])\n",
    "first_non_zero_pressure_timestamp = pd.DataFrame.from_records(loaded_data['first_non_zero_pressure_timestamp'])\n",
    "first_full_pressure_timestamp = pd.DataFrame.from_records(loaded_data['first_full_pressure_timestamp'])\n",
    "\n",
    "with open('simple_stop_data.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "simple_stop_metrics = pd.DataFrame.from_records(loaded_data['simple_stop_metrics'])\n",
    "\n",
    "with open('ai_survey_data.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "survey_scores = convert_formats_back(loaded_data['survey_scores'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Mean and SD rows for Simple Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_stop_ssrt = simple_stop_metrics.iloc[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Simple Stop and Force Sensitive Stopping Task SSRTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(force_sensitive_stopping_task_ssrt, simple_stop_metrics[['ssrt', 'ssrt_without_short_ssd_trials', 'ssrt_without_short_ssd_subs']], \n",
    "                    left_index=True, right_index=True, how='left')\n",
    "merged_df.rename(columns={\n",
    "    'ssrt': 'simple_stop_ssrt',\n",
    "    'ssrt_without_short_ssd_trials': 'simple_stop_ssrt_without_short_ssd_trials',\n",
    "    'ssrt_without_short_ssd_subs': 'simple_stop_ssrt_without_short_ssd_subs'\n",
    "}, inplace=True)\n",
    "\n",
    "rename_index_column(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-failed vs Non-AI SSRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 298.51\n",
      "Mean non_ai: 276.53\n",
      "T-statistic: 5.90, p-value: 0.00\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(merged_df, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d: 1.32\n",
      "Mean difference (ai_failed - non_ai): 21.98 ms\n",
      "95% CI: [14.44, 29.52] ms\n"
     ]
    }
   ],
   "source": [
    "print_effect_size_and_ci(merged_df,'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-assisted vs AI-failed SSRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_assisted: 331.50\n",
      "Mean ai_failed: 298.51\n",
      "T-statistic: 3.84, p-value: 0.00\n",
      "Significant difference (ai_assisted vs ai_failed)? Yes\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(merged_df, 'ai_assisted', 'ai_failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d: 0.86\n",
      "Mean difference (ai_assisted - ai_failed): 33.00 ms\n",
      "95% CI: [15.61, 50.38] ms\n"
     ]
    }
   ],
   "source": [
    "print_effect_size_and_ci(merged_df, 'ai_assisted', 'ai_failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-AI SSRT vs Simple Stop SSRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean non_ai: 276.53\n",
      "Mean simple_stop_ssrt: 210.57\n",
      "T-statistic: 12.83, p-value: 0.00\n",
      "Significant difference (non_ai vs simple_stop_ssrt)? Yes\n",
      "Correlation: 0.46\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(merged_df, 'non_ai', 'simple_stop_ssrt')\n",
    "non_ai_vs_simple_corr = np.corrcoef(merged_df['non_ai'], merged_df['simple_stop_ssrt'])[1][0]\n",
    "print(f\"Correlation: {non_ai_vs_simple_corr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's d: 2.87\n",
      "Mean difference (non_ai - simple_stop_ssrt): 65.96 ms\n",
      "95% CI: [55.56, 76.36] ms\n"
     ]
    }
   ],
   "source": [
    "print_effect_size_and_ci(merged_df, 'non_ai', 'simple_stop_ssrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate order effects between subjects who had the AI block first vs. the Non-AI block first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize subjects who had the Non-AI and AI Blocks first\n",
    "non_ai_first_subs = ['s004', 's009', 's008', 's011', 's012', 's015', 's016', 's019', 's020', 's023', 's024', 's027', 's028', 's031', 's032', 's035', 's036', 's039', 's040', 's043']\n",
    "ai_first_subs = ['s005', 's006', 's007', 's010', 's013', 's014', 's017', 's018', 's021', 's022', 's025', 's026', 's029', 's030', 's033', 's034', 's037', 's038', 's041', 's042']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AI First: 25.85\n",
      "Mean Non-AI First: 18.11\n",
      "T-statistic: 0.86, p-value: 0.40\n",
      "Significant difference (AI First vs Non-AI First)? No\n",
      "Cohen's d: 0.27\n",
      "Mean difference (AI First - Non-AI First): 7.73 ms\n",
      "95% CI: [-11.12, 26.59] ms\n"
     ]
    }
   ],
   "source": [
    "ssrt_non_ai_first_non_ai = force_sensitive_stopping_task_ssrt.loc[non_ai_first_subs, \"non_ai\"].values\n",
    "ssrt_non_ai_first_ai_failed = force_sensitive_stopping_task_ssrt.loc[non_ai_first_subs, \"ai_failed\"].values\n",
    "ssrt_ai_first_non_ai = force_sensitive_stopping_task_ssrt.loc[ai_first_subs, \"non_ai\"].values\n",
    "ssrt_ai_first_ai_failed = force_sensitive_stopping_task_ssrt.loc[ai_first_subs, \"ai_failed\"].values\n",
    "\n",
    "# Calculate differences\n",
    "diff_non_ai_first = ssrt_non_ai_first_ai_failed - ssrt_non_ai_first_non_ai\n",
    "diff_ai_first = ssrt_ai_first_ai_failed - ssrt_ai_first_non_ai\n",
    "\n",
    "# Create DataFrame\n",
    "diff_df = pd.DataFrame({\n",
    "    'Non-AI First': diff_non_ai_first,\n",
    "    'AI First': diff_ai_first,\n",
    "})\n",
    "\n",
    "print_means_t_test(diff_df, 'AI First', 'Non-AI First')\n",
    "print_effect_size_and_ci(diff_df, 'AI First', 'Non-AI First')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate AI-failed vs. Non-AI SSRT depending on whether they occurred in the first half or second half of the block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 296.03\n",
      "Mean non_ai: 273.70\n",
      "T-statistic: 4.06, p-value: 0.00\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: 0.91\n",
      "Mean difference (ai_failed - non_ai): 22.33 ms\n",
      "95% CI: [11.22, 33.44] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(ssrt_first_half, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(ssrt_first_half, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 303.34\n",
      "Mean non_ai: 279.36\n",
      "T-statistic: 5.32, p-value: 0.00\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: 1.19\n",
      "Mean difference (ai_failed - non_ai): 23.98 ms\n",
      "95% CI: [14.86, 33.11] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(ssrt_second_half, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(ssrt_second_half, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory T-tests and Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-AI vs AI-failed Duration of Inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 47.72\n",
      "Mean non_ai: 41.79\n",
      "T-statistic: 2.87, p-value: 0.01\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: 0.64\n",
      "Mean difference (ai_failed - non_ai): 5.93 ms\n",
      "95% CI: [1.76, 10.10] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(duration_of_inhibition, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(duration_of_inhibition, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-AI vs AI-failed Go Task Accuracy Before Stop Onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 0.90\n",
      "Mean non_ai: 0.87\n",
      "T-statistic: 2.81, p-value: 0.01\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: 0.63\n",
      "Mean difference (ai_failed - non_ai): 0.03 ms\n",
      "95% CI: [0.01, 0.06] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(go_task_accuracy_before_stop_onset, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(go_task_accuracy_before_stop_onset, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-AI vs AI-failed Go Task Accuracy After Stop Onset (aka Stop Success Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 0.41\n",
      "Mean non_ai: 0.60\n",
      "T-statistic: -8.88, p-value: 0.00\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: -1.98\n",
      "Mean difference (ai_failed - non_ai): -0.19 ms\n",
      "95% CI: [-0.24, -0.15] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(go_task_accuracy_after_stop_onset, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(go_task_accuracy_after_stop_onset, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-AI vs AI-failed SSRT in subjects who did not show proactive slowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_proactive_slowing_subs = go_task_accuracy_before_stop_onset[\n",
    "    go_task_accuracy_before_stop_onset['non_ai'] > go_task_accuracy_before_stop_onset['ai_failed']\n",
    "].index.tolist()\n",
    "\n",
    "ssrt_data = force_sensitive_stopping_task_ssrt.copy() # Avoid modifying original dataframe\n",
    "ssrt_data = ssrt_data.rename_axis('subject_id').reset_index() #add subject_id column\n",
    "ssrt_data = ssrt_data[ssrt_data['subject_id'] != 'mean'] #remove mean row\n",
    "\n",
    "# Calculate difference and assign slowing type using vectorized operations\n",
    "ssrt_data['difference_ms'] = ssrt_data['ai_failed'] - ssrt_data['non_ai']\n",
    "ssrt_data['slowing_type'] = np.where(ssrt_data['subject_id'].isin(non_proactive_slowing_subs), 'Non-Proactive', 'Proactive')\n",
    "\n",
    "# Filter for non-proactive slowing data\n",
    "non_proactive_slowing_data = ssrt_data[ssrt_data['slowing_type'] == 'Non-Proactive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 296.86\n",
      "Mean non_ai: 267.57\n",
      "T-statistic: 2.84, p-value: 0.02\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: 1.27\n",
      "Mean difference (ai_failed - non_ai): 29.28 ms\n",
      "95% CI: [5.95, 52.62] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(non_proactive_slowing_data, 'ai_failed', 'non_ai')\n",
    "print_effect_size_and_ci(non_proactive_slowing_data, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test comparing AI-failed minus Non-AI SSRT in subjects who proactively slowed and did not proactively slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent samples t-test:\n",
      "  t-statistic = 1.14\n",
      "  p-value = 0.263\n",
      "  Cohen's d = 0.43\n",
      "Mean difference (Non-Proactive): 29.28 ms\n",
      "Mean difference (Proactive): 19.55 ms\n"
     ]
    }
   ],
   "source": [
    "non_proactive_differences = ssrt_data[ssrt_data['slowing_type'] == 'Non-Proactive']['difference_ms'].dropna()\n",
    "proactive_differences = ssrt_data[ssrt_data['slowing_type'] == 'Proactive']['difference_ms'].dropna()\n",
    "\n",
    "calc_stats_ind(non_proactive_differences, proactive_differences)\n",
    "\n",
    "print(f\"Mean difference (Non-Proactive): {ssrt_data[ssrt_data['slowing_type'] == 'Non-Proactive']['difference_ms'].mean():.2f} ms\")\n",
    "print(f\"Mean difference (Proactive): {ssrt_data[ssrt_data['slowing_type'] == 'Proactive']['difference_ms'].mean():.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test comparing the first non-zero pressure timestamp between the AI and Non-AI Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 49.29\n",
      "Mean non_ai: 50.66\n",
      "T-statistic: -0.30, p-value: 0.76\n",
      "Significant difference (ai_failed vs non_ai)? No\n",
      "Cohen's d: -0.07\n",
      "Mean difference (ai_failed - non_ai): -1.38 ms\n",
      "95% CI: [-10.52, 7.77] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(first_non_zero_pressure_timestamp, 'ai_failed', 'non_ai') # Both AI-failed and AI-assisted columns represent the enter AI Block\n",
    "print_effect_size_and_ci(first_non_zero_pressure_timestamp, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test comparing the first full pressure timestamp between the AI and Non-AI Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ai_failed: 148.96\n",
      "Mean non_ai: 177.99\n",
      "T-statistic: -2.33, p-value: 0.03\n",
      "Significant difference (ai_failed vs non_ai)? Yes\n",
      "Cohen's d: -0.52\n",
      "Mean difference (ai_failed - non_ai): -29.03 ms\n",
      "95% CI: [-54.27, -3.79] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(first_full_pressure_timestamp, 'ai_failed', 'non_ai') # Both AI-failed and AI-assisted columns represent the enter AI Block\n",
    "print_effect_size_and_ci(first_full_pressure_timestamp, 'ai_failed', 'non_ai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlate survey scores with SSRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation and p-value between SSRT and survey scores: -0.05, 0.738\n"
     ]
    }
   ],
   "source": [
    "survey_scores_df = pd.DataFrame(survey_scores)\n",
    "avg_df = merged_df.merge(survey_scores_df, on='subject_id', how='left')\n",
    "avg_df['difference_ai_failed_and_non_ai_ssrt'] = avg_df['ai_failed'] - avg_df['non_ai']\n",
    "correlation, pval = stats.pearsonr(avg_df['average_score'], avg_df[\"difference_ai_failed_and_non_ai_ssrt\"])\n",
    "print(f\"Correlation and p-value between SSRT and survey scores: {correlation:.2f}, {pval:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations of Context Independence in Simple Stop Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Non-AI SSRT vs Simple Stop SSRT Excluding trials with SSD < 200 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean non_ai: 276.53\n",
      "Mean simple_stop_ssrt_without_short_ssd_trials: 207.72\n",
      "T-statistic: 12.28, p-value: 0.00\n",
      "Significant difference (non_ai vs simple_stop_ssrt_without_short_ssd_trials)? Yes\n",
      "Cohen's d: 2.74\n",
      "Mean difference (non_ai - simple_stop_ssrt_without_short_ssd_trials): 68.80 ms\n",
      "95% CI: [57.46, 80.14] ms\n"
     ]
    }
   ],
   "source": [
    "print_means_t_test(merged_df, 'non_ai', 'simple_stop_ssrt_without_short_ssd_trials')\n",
    "print_effect_size_and_ci(merged_df, 'non_ai', 'simple_stop_ssrt_without_short_ssd_trials')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Non-AI SSRT vs Simple Stop SSRT Excluding subjects with average SSDs < 200ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean non_ai: 279.43\n",
      "Mean simple_stop_ssrt_without_short_ssd_subs: 208.59\n",
      "T-statistic: 13.34, p-value: 0.00\n",
      "Significant difference (non_ai vs simple_stop_ssrt_without_short_ssd_subs)? Yes\n",
      "Cohen's d: 3.19\n",
      "Mean difference (non_ai - simple_stop_ssrt_without_short_ssd_subs): 70.84 ms\n",
      "95% CI: [60.05, 81.64] ms\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NaN in simple_stop_ssrt_without_short_ssd_subs\n",
    "filtered_df = merged_df.dropna(subset=['simple_stop_ssrt_without_short_ssd_subs'])\n",
    "\n",
    "print_means_t_test(filtered_df, 'non_ai', 'simple_stop_ssrt_without_short_ssd_subs')\n",
    "print_effect_size_and_ci(filtered_df, 'non_ai', 'simple_stop_ssrt_without_short_ssd_subs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store merged_df to be accessed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = {\n",
    "    'merged_df': merged_df.to_dict(),\n",
    "}\n",
    "\n",
    "with open('merged_df.json', 'w') as f:\n",
    "    json.dump(data_to_save, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
