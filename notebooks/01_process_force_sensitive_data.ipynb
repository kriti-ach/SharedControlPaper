{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76d7269-3905-4509-94bc-77e1cca385b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from sharedcontrolpaper.force_sensitive_stopping_task_utils import get_subject_label, string_to_numbers, process_trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb7faae-da47-461d-b5be-9eaa0074b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(parent_directory, 'data', 'experiment')\n",
    "task = \"force_sensitive_stopping_task\"\n",
    "exp_stage = \"final\"\n",
    "pattern = os.path.join(data_path, exp_stage, '*', task, '*.csv')\n",
    "data_files = glob.glob(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d185d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_control_metrics = {}\n",
    "\n",
    "for file in data_files:\n",
    "    subject_label = get_subject_label(file)\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # some post processing\n",
    "    df['block'] = df['block'].str.strip(\"'\")\n",
    "    # create a dataFrame for each block\n",
    "    df_test = df.query(\"block != 'practice'\")\n",
    "    block_1 = df_test.query(\"block == 'block 1'\")\n",
    "    block_2 = df_test.query(\"block == 'block 2'\")\n",
    "\n",
    "    block_1 = block_1.reset_index(drop=True)\n",
    "    block_2 = block_2.reset_index(drop=True)\n",
    "\n",
    "    task_dfs = [block_1, block_2]\n",
    "    \n",
    "    for df in task_dfs:\n",
    "        if 'AI-engaged' in df['condition'].values:\n",
    "            ai_data = df.copy()\n",
    "            ai_data['distances'] = ai_data['distances'].apply(string_to_numbers)\n",
    "            ai_data['pressures'] = ai_data['pressures'].apply(string_to_numbers)\n",
    "            ai_data['time_stamps'] = ai_data['time_stamps'].apply(string_to_numbers)\n",
    "        else:\n",
    "            non_ai_data = df.copy()\n",
    "            non_ai_data['distances'] = non_ai_data['distances'].apply(string_to_numbers)\n",
    "            non_ai_data['pressures'] = non_ai_data['pressures'].apply(string_to_numbers)\n",
    "            non_ai_data['time_stamps'] = non_ai_data['time_stamps'].apply(string_to_numbers)\n",
    "    \n",
    "    shared_control_metrics[subject_label] = {'AI': {'data': ai_data}, 'Non-AI': {'data': non_ai_data}}\n",
    "    \n",
    "    for block in shared_control_metrics[subject_label].keys():\n",
    "        trial_results, ssrt_list = process_trial_data(shared_control_metrics[subject_label][block]['data'], block=block)\n",
    "        shared_control_metrics[subject_label][block]['trial_results'] = trial_results\n",
    "        shared_control_metrics[subject_label][block]['ssrt_list'] = ssrt_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5c6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'shared_control_metrics' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store shared_control_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3e7cb",
   "metadata": {},
   "source": [
    "## Excluded Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e8c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = {\"s027\": [\"AI\", 80, 96]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77167a21-0510-4e0a-9670-eb5572056616",
   "metadata": {},
   "source": [
    "## Grabbing SSRT and other metrics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784fcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_mean_metric(measure):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to find the mean of a specified metric across different trial conditions \n",
    "    for each subject. The provided measure should be a string representing the metric \n",
    "    to analyze (e.g., 'ssrt' or 'duration_of_inhibition'). The results are saved to a \n",
    "    CSV file with each row corresponding to a subject, with columns for the means \n",
    "    of the measure in each of the three trial conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - measure (str): The name of the measure to calculate the mean for.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves a CSV file with means for each subject across non_ai trials, ai_condition_stop_trials, \n",
    "      and ai_condition_ai_trials, as well as handling specific conditions based on the flag.\n",
    "    \"\"\"\n",
    "\n",
    "    condition_measure = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        non_ai_trials = []\n",
    "        ai_disengaged_trials = []\n",
    "        ai_engaged_trials = []\n",
    "        \n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            \n",
    "            for trial in shared_control_metrics[subject][block]['trial_results'].keys():\n",
    "\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "        \n",
    "\n",
    "                if block == 'Non-AI':\n",
    "                    non_ai_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "                elif (block == 'AI') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'AI-disengaged'):\n",
    "                    ai_disengaged_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "                elif (block == 'AI') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'AI-engaged'):\n",
    "                    ai_engaged_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "\n",
    "        avg_ai_engaged = np.nanmean(ai_engaged_trials)\n",
    "        avg_ai_disengaged = np.nanmean(ai_disengaged_trials)\n",
    "        avg_non_ai = np.nanmean(non_ai_trials)  \n",
    "        \n",
    "        condition_measure[subject] = {f'non_ai': avg_non_ai, f'ai_disengaged': avg_ai_disengaged, f'ai_engaged': avg_ai_engaged}\n",
    "\n",
    "    df = pd.DataFrame(condition_measure).T\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e319d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'force_sensitive_stopping_task_ssrt' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "force_sensitive_stopping_task_ssrt = grab_mean_metric('ssrt')\n",
    "%store force_sensitive_stopping_task_ssrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90eff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'duration_of_inhibition' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "duration_of_inhibition = grab_mean_metric('duration_of_inhibition')\n",
    "%store duration_of_inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00356be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'go_task_accuracy_before_stop_onset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "go_task_accuracy_before_stop_onset = grab_mean_metric('go_task_accuracy_before_stop_onset')\n",
    "%store go_task_accuracy_before_stop_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be174f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'go_task_accuracy_after_stop_onset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "go_task_accuracy_after_stop_onset = grab_mean_metric('go_task_accuracy_after_stop_onset')\n",
    "%store go_task_accuracy_after_stop_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524b814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        non_ai  ai_disengaged  ai_engaged\n",
      "s043  0.209611        0.32973    0.173612\n"
     ]
    }
   ],
   "source": [
    "ball_before_ring_proportion_before_stop_onset = grab_mean_metric('ball_before_ring_proportion_before_stop_onset')\n",
    "print(ball_before_ring_proportion_before_stop_onset.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dfa308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      non_ai  ai_disengaged  ai_engaged\n",
      "s043     0.0            0.0         0.0\n"
     ]
    }
   ],
   "source": [
    "ball_after_ring_proportion_before_stop_onset = grab_mean_metric('ball_after_ring_proportion_before_stop_onset')\n",
    "print(ball_after_ring_proportion_before_stop_onset.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6e8f7",
   "metadata": {},
   "source": [
    "## Finding the proportion of trials where subjects inhibited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e19bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportion_metric(metric):\n",
    "    proportion = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        count_non_ai = 0\n",
    "        count_ai_disengaged = 0\n",
    "        count_ai_engaged = 0\n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            \n",
    "            for trial in shared_control_metrics[subject][block]['trial_results'].keys():\n",
    "\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "\n",
    "                if (block == 'Non-AI'):\n",
    "                    if not np.isnan(shared_control_metrics[subject][block]['trial_results'][trial][metric]):\n",
    "                        count_non_ai += 1\n",
    "                \n",
    "                elif (block == 'AI') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'AI-disengaged'):\n",
    "                    if not np.isnan(shared_control_metrics[subject][block]['trial_results'][trial][metric]):\n",
    "                        count_ai_disengaged += 1\n",
    "                        \n",
    "                elif (block == 'AI') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'AI-engaged'):\n",
    "                    if not np.isnan(shared_control_metrics[subject][block]['trial_results'][trial][metric]):\n",
    "                        count_ai_engaged += 1\n",
    "\n",
    "        proportions_non_ai = count_non_ai / 100\n",
    "        proportions_ai_disengaged = count_ai_disengaged / 20\n",
    "        proportions_ai_engaged = count_ai_engaged / 80\n",
    "        \n",
    "        proportion[subject] = {\n",
    "            'proportion_non_ai': proportions_non_ai,\n",
    "            'proportion_ai_disengaged': proportions_ai_disengaged,\n",
    "            'proportion_ai_engaged': proportions_ai_engaged\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(proportion).T\n",
    "    df = df.sort_index()\n",
    "    df.loc['mean'] = df.mean()\n",
    "    print(df.loc['mean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81402972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion_non_ai           0.999750\n",
      "proportion_ai_disengaged    0.996250\n",
      "proportion_ai_engaged       0.989062\n",
      "Name: mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "find_proportion_metric('ssrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff2075",
   "metadata": {},
   "source": [
    "## Create CSVs of SSRT by each half of trials in a block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78161d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_mean_metric_by_halves(measure):\n",
    "    \"\"\"\n",
    "    Function to find the mean of a specified metric across different trial conditions \n",
    "    for each subject, split by halves of trials.\n",
    "\n",
    "    Parameters:\n",
    "    - measure (str): The name of the measure to calculate the mean for.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves two CSV files with means for each subject across non_ai trials and ai condition trials\n",
    "      split by halves.\n",
    "    \"\"\"\n",
    "    # Initialize structures for data collection\n",
    "    condition_measure_first_half = {}\n",
    "    condition_measure_second_half = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        non_ai_first_half = []\n",
    "        non_ai_second_half = []\n",
    "        ai_disengaged_first_half = []\n",
    "        ai_disengaged_second_half = []\n",
    "        ai_engaged_first_half = []\n",
    "        ai_engaged_second_half = []\n",
    "        \n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            trial_results = shared_control_metrics[subject][block]['trial_results']\n",
    "            num_trials = len(trial_results)\n",
    "\n",
    "            for index, trial in enumerate(trial_results.keys()):\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "                \n",
    "                ssrt_value = shared_control_metrics[subject][block]['trial_results'][trial][measure]\n",
    "                \n",
    "                if pd.isna(ssrt_value):\n",
    "                    continue\n",
    "                \n",
    "                if block == 'Non-AI':\n",
    "                    if index < num_trials / 2:  # First half\n",
    "                        non_ai_first_half.append(ssrt_value)\n",
    "                    else:  # Second half\n",
    "                        non_ai_second_half.append(ssrt_value)\n",
    "                    \n",
    "                elif block == 'AI':\n",
    "                    condition = shared_control_metrics[subject][block]['trial_results'][trial]['condition']\n",
    "                    if condition == 'AI-disengaged':\n",
    "                        if index < num_trials / 2:  # First half\n",
    "                            ai_disengaged_first_half.append(ssrt_value)\n",
    "                        else:  # Second half\n",
    "                            ai_disengaged_second_half.append(ssrt_value)\n",
    "                    elif condition == 'AI-engaged':\n",
    "                        if index < num_trials / 2:  # First half\n",
    "                            ai_engaged_first_half.append(ssrt_value)\n",
    "                        else:  # Second half\n",
    "                            ai_engaged_second_half.append(ssrt_value)\n",
    "                            \n",
    "        condition_measure_first_half[subject] = {\n",
    "            'non_ai': np.nanmean(non_ai_first_half),\n",
    "            'ai_disengaged': np.nanmean(ai_disengaged_first_half),\n",
    "            'ai_engaged': np.nanmean(ai_engaged_first_half)\n",
    "        }\n",
    "        \n",
    "        condition_measure_second_half[subject] = {\n",
    "            'non_ai': np.nanmean(non_ai_second_half),\n",
    "            'ai_disengaged': np.nanmean(ai_disengaged_second_half),\n",
    "            'ai_engaged': np.nanmean(ai_engaged_second_half)\n",
    "        }\n",
    "    \n",
    "    df_first_half = pd.DataFrame(condition_measure_first_half).T\n",
    "    df_second_half = pd.DataFrame(condition_measure_second_half).T\n",
    "\n",
    "    df_first_half = df_first_half.sort_index()\n",
    "\n",
    "    df_second_half = df_second_half.sort_index()\n",
    "    \n",
    "    return df_first_half, df_second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e23021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ssrt_first_half' (DataFrame)\n",
      "Stored 'ssrt_second_half' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "ssrt_first_half, ssrt_second_half = grab_mean_metric_by_halves('ssrt')\n",
    "%store ssrt_first_half\n",
    "%store ssrt_second_half"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
