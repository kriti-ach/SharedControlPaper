{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76d7269-3905-4509-94bc-77e1cca385b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from sharedcontrolpaper.force_sensitive_stopping_task_utils import get_subject_label, string_to_numbers, process_trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb7faae-da47-461d-b5be-9eaa0074b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(parent_directory, 'data', 'experiment')\n",
    "task = \"shared_control\"\n",
    "exp_stage = \"final\"\n",
    "pattern = os.path.join(data_path, exp_stage, '*', task, '*.csv')\n",
    "data_files = glob.glob(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d185d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_control_metrics = {}\n",
    "\n",
    "for file in data_files:\n",
    "    subject_label = get_subject_label(file)\n",
    "    \n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # some post processing\n",
    "    df['phase_raw'] = df['phase_raw'].str.strip(\"'\")\n",
    "    df['block_raw'] = df['block_raw'].str.strip(\"'\")\n",
    "    \n",
    "    df_test = df.query(\"phase_raw == 'test'\")\n",
    "    block_1 = df_test.query(\"block_raw == 'block 1'\")\n",
    "    block_2 = df_test.query(\"block_raw == 'block 2'\")\n",
    "    block_1 = block_1.reset_index(drop=True)\n",
    "    block_2 = block_2.reset_index(drop=True)\n",
    "\n",
    "    task_dfs = [block_1, block_2]\n",
    "    \n",
    "    for df in task_dfs:\n",
    "        if 'ai' in df['condition'].values:\n",
    "            ai_data = df.copy()\n",
    "            ai_data['distances_raw'] = ai_data['distances_raw'].apply(string_to_numbers)\n",
    "            ai_data['pressures_raw'] = ai_data['pressures_raw'].apply(string_to_numbers)\n",
    "            ai_data['time_stamps_raw'] = ai_data['time_stamps_raw'].apply(string_to_numbers)\n",
    "        else:\n",
    "            control_data = df.copy()\n",
    "            control_data['distances_raw'] = control_data['distances_raw'].apply(string_to_numbers)\n",
    "            control_data['pressures_raw'] = control_data['pressures_raw'].apply(string_to_numbers)\n",
    "            control_data['time_stamps_raw'] = control_data['time_stamps_raw'].apply(string_to_numbers)\n",
    "    \n",
    "    shared_control_metrics[subject_label] = {'ai': {'data': ai_data}, 'non_ai': {'data': control_data}}\n",
    "    \n",
    "    for block in shared_control_metrics[subject_label].keys():\n",
    "        trial_results, ssrt_list = process_trial_data(shared_control_metrics[subject_label][block]['data'], block=block)\n",
    "        shared_control_metrics[subject_label][block]['trial_results'] = trial_results\n",
    "        shared_control_metrics[subject_label][block]['ssrt_list'] = ssrt_list \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5c6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'shared_control_metrics' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store shared_control_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb3e7cb",
   "metadata": {},
   "source": [
    "## Excluded Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e8c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusions = {\"s027\": [\"ai\", 80, 96]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77167a21-0510-4e0a-9670-eb5572056616",
   "metadata": {},
   "source": [
    "## Grabbing SSRT and other metrics across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "784fcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_mean_metric(measure):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to find the mean of a specified metric across different trial conditions \n",
    "    for each subject. The provided measure should be a string representing the metric \n",
    "    to analyze (e.g., 'ssrt' or 'duration_of_inhibition'). The results are saved to a \n",
    "    CSV file with each row corresponding to a subject, with columns for the means \n",
    "    of the measure in each of the three trial conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - measure (str): The name of the measure to calculate the mean for.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves a CSV file with means for each subject across non_ai trials, ai_condition_stop_trials, \n",
    "      and ai_condition_ai_trials, as well as handling specific conditions based on the flag.\n",
    "    \"\"\"\n",
    "\n",
    "    condition_measure = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        non_ai_condition_stop_trials = []\n",
    "        ai_condition_stop_trials = []\n",
    "        ai_condition_ai_trials = []\n",
    "        \n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            \n",
    "            for trial in shared_control_metrics[subject][block]['trial_results'].keys():\n",
    "\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "        \n",
    "\n",
    "                if block == 'non_ai':\n",
    "                    non_ai_condition_stop_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "                elif (block == 'ai') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'stop'):\n",
    "                    ai_condition_stop_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "                elif (block == 'ai') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'ai'):\n",
    "                    ai_condition_ai_trials.append(shared_control_metrics[subject][block]['trial_results'][trial][measure])\n",
    "                    \n",
    "\n",
    "        avg_ai_ai = np.nanmean(ai_condition_ai_trials)\n",
    "        avg_ai = np.nanmean(ai_condition_stop_trials)\n",
    "        avg_non_ai = np.nanmean(non_ai_condition_stop_trials)\n",
    "        \n",
    "        condition_measure[subject] = {f'non_ai': avg_non_ai, f'ai': avg_ai, f'ai_ai': avg_ai_ai}\n",
    "\n",
    "    df = pd.DataFrame(condition_measure).T\n",
    "    df = df.sort_index()\n",
    "    df.loc['mean'] = df.mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e319d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'shared_control_ssrt' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "shared_control_ssrt = grab_mean_metric('ssrt')\n",
    "%store shared_control_ssrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90eff66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'duration_of_inhibition' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "duration_of_inhibition = grab_mean_metric('duration_of_inhibition')\n",
    "%store duration_of_inhibition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00356be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'go_task_accuracy_before_stop_onset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "go_task_accuracy_before_stop_onset = grab_mean_metric('go_task_accuracy_before_stop_onset')\n",
    "%store go_task_accuracy_before_stop_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be174f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'go_task_accuracy_after_stop_onset' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "go_task_accuracy_after_stop_onset = grab_mean_metric('go_task_accuracy_after_stop_onset')\n",
    "%store go_task_accuracy_after_stop_onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "524b814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        non_ai        ai     ai_ai\n",
      "mean  0.133025  0.099186  0.096252\n"
     ]
    }
   ],
   "source": [
    "ball_before_ring_proportion_before_stop_onset = grab_mean_metric('ball_before_ring_proportion_before_stop_onset')\n",
    "print(ball_before_ring_proportion_before_stop_onset.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dfa308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        non_ai        ai     ai_ai\n",
      "mean  0.000177  0.000093  0.000235\n"
     ]
    }
   ],
   "source": [
    "ball_after_ring_proportion_before_stop_onset = grab_mean_metric('ball_after_ring_proportion_before_stop_onset')\n",
    "print(ball_after_ring_proportion_before_stop_onset.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a6e8f7",
   "metadata": {},
   "source": [
    "## Finding the proportion of trials where subjects inhibited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e19bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_proportion_metric(metric):\n",
    "    proportion = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        count_non_ai = 0\n",
    "        count_ai = 0\n",
    "        count_ai_ai = 0\n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            \n",
    "            for trial in shared_control_metrics[subject][block]['trial_results'].keys():\n",
    "\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "\n",
    "                if (block == 'non_ai'):\n",
    "                    if shared_control_metrics[subject][block]['trial_results'][trial][metric] != None:\n",
    "                        count_non_ai += 1\n",
    "                \n",
    "                elif (block == 'ai') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'stop'):\n",
    "                    if shared_control_metrics[subject][block]['trial_results'][trial][metric] != None:\n",
    "                        count_ai += 1\n",
    "                        \n",
    "                elif (block == 'ai') and (shared_control_metrics[subject][block]['trial_results'][trial]['condition'] == 'ai'):\n",
    "                    if shared_control_metrics[subject][block]['trial_results'][trial][metric] != None:\n",
    "                        count_ai_ai += 1\n",
    "\n",
    "        proportions_non_ai = count_non_ai / 100\n",
    "        proportions_ai = count_ai / 20\n",
    "        proportions_ai_ai = count_ai_ai / 80\n",
    "        \n",
    "        proportion[subject] = {\n",
    "            'proportion_non_ai': proportions_non_ai,\n",
    "            'proportion_ai': proportions_ai,\n",
    "            'proportion_ai_ai': proportions_ai_ai\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(proportion).T\n",
    "    df = df.sort_index()\n",
    "    df.loc['mean'] = df.mean()\n",
    "    print(df.loc['mean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81402972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion_non_ai    0.999750\n",
      "proportion_ai        0.996250\n",
      "proportion_ai_ai     0.989375\n",
      "Name: mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "find_proportion_metric('moment_of_inhibition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ff2075",
   "metadata": {},
   "source": [
    "## Create CSVs of SSRT by each half of trials in a block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78161d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_mean_metric_by_halves(measure):\n",
    "    \"\"\"\n",
    "    Function to find the mean of a specified metric across different trial conditions \n",
    "    for each subject, split by halves of trials.\n",
    "\n",
    "    Parameters:\n",
    "    - measure (str): The name of the measure to calculate the mean for.\n",
    "\n",
    "    Outputs:\n",
    "    - Saves two CSV files with means for each subject across non_ai trials and ai condition trials\n",
    "      split by halves.\n",
    "    \"\"\"\n",
    "    # Initialize structures for data collection\n",
    "    condition_measure_first_half = {}\n",
    "    condition_measure_second_half = {}\n",
    "\n",
    "    for subject in shared_control_metrics.keys():\n",
    "        non_ai_condition_stop_trials_first_half = []\n",
    "        non_ai_condition_stop_trials_second_half = []\n",
    "        ai_condition_stop_trials_first_half = []\n",
    "        ai_condition_ai_trials_first_half = []\n",
    "        ai_condition_stop_trials_second_half = []\n",
    "        ai_condition_ai_trials_second_half = []\n",
    "        \n",
    "        for block in shared_control_metrics[subject].keys():\n",
    "            trial_results = shared_control_metrics[subject][block]['trial_results']\n",
    "            num_trials = len(trial_results)\n",
    "\n",
    "            for index, trial in enumerate(trial_results.keys()):\n",
    "                if subject in exclusions.keys() and trial in exclusions[subject] and block in exclusions[subject]:\n",
    "                    continue\n",
    "                \n",
    "                ssrt_value = shared_control_metrics[subject][block]['trial_results'][trial][measure]\n",
    "                \n",
    "                if pd.isna(ssrt_value):\n",
    "                    continue\n",
    "                \n",
    "                if block == 'non_ai':\n",
    "                    if index < num_trials / 2:  # First half\n",
    "                        non_ai_condition_stop_trials_first_half.append(ssrt_value)\n",
    "                    else:  # Second half\n",
    "                        non_ai_condition_stop_trials_second_half.append(ssrt_value)\n",
    "                    \n",
    "                elif block == 'ai':\n",
    "                    condition = shared_control_metrics[subject][block]['trial_results'][trial]['condition']\n",
    "                    if condition == 'stop':\n",
    "                        if index < num_trials / 2:  # First half\n",
    "                            ai_condition_stop_trials_first_half.append(ssrt_value)\n",
    "                        else:  # Second half\n",
    "                            ai_condition_stop_trials_second_half.append(ssrt_value)\n",
    "                    elif condition == 'ai':\n",
    "                        if index < num_trials / 2:  # First half\n",
    "                            ai_condition_ai_trials_first_half.append(ssrt_value)\n",
    "                        else:  # Second half\n",
    "                            ai_condition_ai_trials_second_half.append(ssrt_value)\n",
    "                            \n",
    "        condition_measure_first_half[subject] = {\n",
    "            'non_ai_ssrt_first_half': np.nanmean(non_ai_condition_stop_trials_first_half),\n",
    "            'ai_ssrt_first_half': np.nanmean(ai_condition_stop_trials_first_half),\n",
    "            'ai_ai_ssrt_first_half': np.nanmean(ai_condition_ai_trials_first_half)\n",
    "        }\n",
    "        \n",
    "        condition_measure_second_half[subject] = {\n",
    "            'non_ai_ssrt_second_half': np.nanmean(non_ai_condition_stop_trials_second_half),\n",
    "            'ai_ssrt_second_half': np.nanmean(ai_condition_stop_trials_second_half),\n",
    "            'ai_ai_ssrt_second_half': np.nanmean(ai_condition_ai_trials_second_half)\n",
    "        }\n",
    "    \n",
    "    df_first_half = pd.DataFrame(condition_measure_first_half).T\n",
    "    df_second_half = pd.DataFrame(condition_measure_second_half).T\n",
    "\n",
    "    df_first_half = df_first_half.sort_index()\n",
    "    df_first_half.loc['mean'] = df_first_half.mean()\n",
    "\n",
    "    df_second_half = df_second_half.sort_index()\n",
    "    df_second_half.loc['mean'] = df_second_half.mean()\n",
    "    \n",
    "    return df_first_half, df_second_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e23021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ssrt_first_half' (DataFrame)\n",
      "Stored 'ssrt_second_half' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "ssrt_first_half, ssrt_second_half = grab_mean_metric_by_halves('ssrt')\n",
    "%store ssrt_first_half\n",
    "%store ssrt_second_half"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
